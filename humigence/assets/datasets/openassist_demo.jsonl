{"messages":[{"role":"system","content":"You are a helpful assistant."},{"role":"user","content":"What is QLoRA?"},{"role":"assistant","content":"QLoRA is a technique that fine-tunes a quantized base model using LoRA adapters to reduce memory usage."}]}
{"messages":[{"role":"system","content":"You are a helpful assistant."},{"role":"user","content":"Explain LoRA in one sentence."},{"role":"assistant","content":"LoRA injects small low‑rank trainable matrices into frozen weights to efficiently adapt large models."}]}
{"messages":[{"role":"system","content":"You are a helpful assistant."},{"role":"user","content":"Give me 3 benefits of quantization."},{"role":"assistant","content":"Lower memory, faster throughput, and cheaper hardware requirements."}]}
{"messages":[{"role":"system","content":"You are a helpful assistant."},{"role":"user","content":"What is bf16?"},{"role":"assistant","content":"bfloat16 is a 16‑bit floating‑point format with a wide exponent range, good for training stability."}]}
{"messages":[{"role":"system","content":"You are a helpful assistant."},{"role":"user","content":"When would I avoid packing sequences?"},{"role":"assistant","content":"Avoid packing for tasks where sample boundaries matter for metrics or your trainer can't mask packed tokens."}]}
{"messages":[{"role":"system","content":"You are a helpful assistant."},{"role":"user","content":"How do I pick LoRA target modules?"},{"role":"assistant","content":"Common choices are q_proj, k_proj, v_proj, o_proj, up_proj, down_proj, and gate_proj for transformer blocks."}]}
{"messages":[{"role":"system","content":"You are a helpful assistant."},{"role":"user","content":"What's a reasonable max sequence length for small cards?"},{"role":"assistant","content":"1024 is a solid default for quick experiments; raise to 2k–4k if you have VRAM and long contexts."}]}
{"messages":[{"role":"system","content":"You are a helpful assistant."},{"role":"user","content":"What's curated eval?"},{"role":"assistant","content":"A small hand‑picked set of prompts checked after training to gauge practical quality beyond loss metrics."}]}
{"messages":[{"role":"system","content":"You are a helpful assistant."},{"role":"user","content":"What should my acceptance gates include?"},{"role":"assistant","content":"Val‑loss improvement, throughput jitter bounds, and a curated prompt threshold are typical."}]}
{"messages":[{"role":"system","content":"You are a helpful assistant."},{"role":"user","content":"Why dry‑run planning first?"},{"role":"assistant","content":"It validates paths and computes an execution plan before spending GPU time."}]}
{"messages":[{"role":"system","content":"You are a helpful assistant."},{"role":"user","content":"What does tokens per step target mean?"},{"role":"assistant","content":"It's the target effective tokens per optimizer step; auto‑VRAM fitting adjusts micro‑batch and grad‑accum."}]}
{"messages":[{"role":"system","content":"You are a helpful assistant."},{"role":"user","content":"What is Humigence?"},{"role":"assistant","content":"A production‑grade QLoRA training pipeline for local GPUs with a validation‑first, UI‑ready design."}]}
